{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-19T21:47:20.852350Z",
     "start_time": "2025-08-19T21:47:09.291837Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sympy.printing.pytorch import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "from config import *\n",
    "from CProGAN import *"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniilogorodnikov/PycharmProjects/Text2Image/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T21:47:20.861102Z",
     "start_time": "2025-08-19T21:47:20.856476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_fn(\n",
    "        critic,\n",
    "        gen,\n",
    "        loader,\n",
    "        dataset,\n",
    "        step,\n",
    "        alpha,\n",
    "        opt_critic,\n",
    "        opt_gen,\n",
    "        tensorboard_step,\n",
    "        writer,\n",
    "        scaler_gen,\n",
    "        scaler_critic,\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, data in enumerate(loop):\n",
    "        real = data['pix'].to(config.DEVICE)\n",
    "        embed = data['emb'].to(config.DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        # Train Critic: max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n",
    "        # which is equivalent to minimizing the negative of the expression\n",
    "        noise = torch.randn(cur_batch_size, config.Z_DIM, 1, 1).to(config.DEVICE)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise, embed, alpha, step)\n",
    "            critic_real = critic(real, embed, alpha, step)\n",
    "            critic_fake = critic(fake.detach(), embed, alpha, step)\n",
    "            gp = gradient_penalty(critic, embed, real, fake, alpha, step, device=config.DEVICE)\n",
    "            loss_critic = (\n",
    "                    -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "                    + config.LAMBDA_GP * gp\n",
    "                    + (0.001 * torch.mean(critic_real ** 2))\n",
    "            )\n",
    "\n",
    "        opt_critic.zero_grad()\n",
    "        scaler_critic.scale(loss_critic).backward()\n",
    "        scaler_critic.step(opt_critic)\n",
    "        scaler_critic.update()\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = critic(fake, embed, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        # Update alpha and ensure less than 1\n",
    "        alpha += cur_batch_size / (\n",
    "                (config.PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(config.FIXED_NOISE, embed, alpha, step) * 0.5 + 0.5\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                loss_critic.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tensorboard_step,\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp=gp.item(),\n",
    "            loss_critic=loss_critic.item(),\n",
    "        )\n",
    "\n",
    "    return tensorboard_step, alpha"
   ],
   "id": "b44045d51c7a986c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T21:47:20.871062Z",
     "start_time": "2025-08-19T21:47:20.867569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    # initialize gen and disc, note: discriminator should be called critic,\n",
    "    # according to WGAN paper (since it no longer outputs between [0, 1])\n",
    "    # but really who cares..\n",
    "    gen = Generator(\n",
    "        config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG\n",
    "    ).to(config.DEVICE)\n",
    "    critic = Discriminator(\n",
    "        config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG\n",
    "    ).to(config.DEVICE)\n",
    "\n",
    "    # initialize optimizers and scalers for FP16 training\n",
    "    opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99))\n",
    "    opt_critic = optim.Adam(\n",
    "        critic.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99)\n",
    "    )\n",
    "    scaler_critic = torch.cuda.amp.GradScaler()\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # for tensorboard plotting\n",
    "    writer = SummaryWriter(f\"logs/gan{config.ITER}\")\n",
    "\n",
    "    if config.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_GEN, gen, opt_gen, config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            config.CHECKPOINT_CRITIC, critic, opt_critic, config.LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "\n",
    "    tensorboard_step = 0\n",
    "    # start at step that corresponds to img size that we set in config\n",
    "    step = int(log2(config.START_TRAIN_AT_IMG_SIZE / 4))\n",
    "    for num_epochs in config.PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-5  # start with very low alpha\n",
    "        loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n",
    "        print(f\"Current image size: {4 * 2 ** step}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "            tensorboard_step, alpha = train_fn(\n",
    "                critic,\n",
    "                gen,\n",
    "                loader,\n",
    "                dataset,\n",
    "                step,\n",
    "                alpha,\n",
    "                opt_critic,\n",
    "                opt_gen,\n",
    "                tensorboard_step,\n",
    "                writer,\n",
    "                scaler_gen,\n",
    "                scaler_critic,\n",
    "            )\n",
    "\n",
    "            if config.SAVE_MODEL:\n",
    "                save_checkpoint(gen, opt_gen, filename=config.CHECKPOINT_GEN)\n",
    "                save_checkpoint(critic, opt_critic, filename=config.CHECKPOINT_CRITIC)\n",
    "\n",
    "        step += 1  # progress to the next img size"
   ],
   "id": "76d1cb1cc6299f94",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T21:57:59.827625Z",
     "start_time": "2025-08-19T21:47:20.876828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "b4b3e307d86b1903",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/48/wvfw_hqj02x66nvb7tyts8bw0000gn/T/ipykernel_41328/3871054805.py:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_critic = torch.cuda.amp.GradScaler()\n",
      "/Users/daniilogorodnikov/PycharmProjects/Text2Image/.venv/lib/python3.13/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/var/folders/48/wvfw_hqj02x66nvb7tyts8bw0000gn/T/ipykernel_41328/3871054805.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_gen = torch.cuda.amp.GradScaler()\n",
      "/Users/daniilogorodnikov/PycharmProjects/Text2Image/.venv/lib/python3.13/site-packages/datasets/load.py:1429: FutureWarning: The repository for poloclub/diffusiondb contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/poloclub/diffusiondb\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image size: 8\n",
      "Epoch [1/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/625 [00:00<?, ?it/s]/var/folders/48/wvfw_hqj02x66nvb7tyts8bw0000gn/T/ipykernel_41328/3494125639.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/Users/daniilogorodnikov/PycharmProjects/Text2Image/.venv/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/var/folders/48/wvfw_hqj02x66nvb7tyts8bw0000gn/T/ipykernel_41328/3494125639.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "100%|██████████| 625/625 [00:16<00:00, 38.21it/s, gp=0.0121, loss_critic=1.38]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [2/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:23<00:00, 26.54it/s, gp=0.00982, loss_critic=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [3/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:16<00:00, 36.96it/s, gp=0.0064, loss_critic=1.52]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [4/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:16<00:00, 38.84it/s, gp=0.00666, loss_critic=0.605]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [5/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:18<00:00, 33.91it/s, gp=0.00937, loss_critic=0.84]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "Epoch [6/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:19<00:00, 31.47it/s, gp=0.00305, loss_critic=0.581]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m\"\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 61\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     59\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m config.SAVE_MODEL:\n\u001B[32m     60\u001B[39m         save_checkpoint(gen, opt_gen, filename=config.CHECKPOINT_GEN)\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m         \u001B[43msave_checkpoint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcritic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopt_critic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCHECKPOINT_CRITIC\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     63\u001B[39m step += \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Text2Image/utils.py:59\u001B[39m, in \u001B[36msave_checkpoint\u001B[39m\u001B[34m(model, optimizer, filename)\u001B[39m\n\u001B[32m     54\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=> Saving checkpoint\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     55\u001B[39m checkpoint = {\n\u001B[32m     56\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mstate_dict\u001B[39m\u001B[33m\"\u001B[39m: model.state_dict(),\n\u001B[32m     57\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33moptimizer\u001B[39m\u001B[33m\"\u001B[39m: optimizer.state_dict(),\n\u001B[32m     58\u001B[39m }\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Text2Image/.venv/lib/python3.13/site-packages/torch/serialization.py:967\u001B[39m, in \u001B[36msave\u001B[39m\u001B[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[39m\n\u001B[32m    965\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[32m    966\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[32m--> \u001B[39m\u001B[32m967\u001B[39m         \u001B[43m_save\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m            \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m            \u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpickle_protocol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m            \u001B[49m\u001B[43m_disable_byteorder_record\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    974\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m    975\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Text2Image/.venv/lib/python3.13/site-packages/torch/serialization.py:1266\u001B[39m, in \u001B[36m_save\u001B[39m\u001B[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001B[39m\n\u001B[32m   1264\u001B[39m         storage = new_storage\n\u001B[32m   1265\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1266\u001B[39m         storage = \u001B[43mstorage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1267\u001B[39m \u001B[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001B[39;00m\n\u001B[32m   1268\u001B[39m zip_file.write_record(name, storage, num_bytes)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Text2Image/.venv/lib/python3.13/site-packages/torch/storage.py:266\u001B[39m, in \u001B[36m_StorageBase.cpu\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    264\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001B[39;00m\n\u001B[32m    265\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device.type != \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m266\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcopy_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gen = Generator(\n",
    "    config.Z_DIM, config.IN_CHANNELS, img_channels=config.CHANNELS_IMG\n",
    ").to(config.DEVICE)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=config.LEARNING_RATE, betas=(0.0, 0.99))"
   ],
   "id": "1528da97b2cd707f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "load_checkpoint(\n",
    "    config.CHECKPOINT_GEN, gen, opt_gen, config.LEARNING_RATE,\n",
    ")"
   ],
   "id": "f7c4f8f06c86e506",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gen.train()",
   "id": "e850e0169fd8fa93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "embedder = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)",
   "id": "af9da7984da3b3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "embeding = torch.from_numpy(embedder.encode(\"tree\")).unsqueeze(0).to(DEVICE)",
   "id": "e67bb8ef6f658a88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "x = torch.randn(1, Z_DIM, 1, 1).to(DEVICE)",
   "id": "675b0ae576f70aa9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "out = gen(x, embeding, alpha = 1e-5, steps = int(log2(config.START_TRAIN_AT_IMG_SIZE / 4)))",
   "id": "3d3acadaa2eaaa64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "out.shape",
   "id": "b112b7ace1967053",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image_from_tensor(tensor: torch.Tensor) -> None:\n",
    "    \"\"\"\n",
    "    Displays an image from a PyTorch tensor of shape [1, 3, 128, 128].\n",
    "\n",
    "    This function assumes the tensor values are in the range [0, 1] or [-1, 1].\n",
    "    It normalizes the tensor if necessary and displays the image using matplotlib.\n",
    "\n",
    "    Args:\n",
    "        tensor (torch.Tensor): The input tensor representing the image.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the tensor does not match the expected shape.\n",
    "    \"\"\"\n",
    "    if tensor.shape != torch.Size([1, 3, 128, 128]):\n",
    "        raise ValueError(f\"Expected tensor shape [1, 3, 128, 128], but got {tensor.shape}\")\n",
    "\n",
    "    # Squeeze the batch dimension\n",
    "    image = tensor.squeeze(0)\n",
    "\n",
    "    # Transpose to [H, W, C] for matplotlib\n",
    "    image = image.permute(1, 2, 0).detach().cpu().numpy()\n",
    "\n",
    "    # Normalize if values are in [-1, 1]\n",
    "    if image.min() < 0:\n",
    "        image = (image + 1) / 2\n",
    "\n",
    "    # Clip values to [0, 1]\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "id": "6cd08afcd3ed13ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display_image_from_tensor(out)",
   "id": "b7b24f53acc92b7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "420c9e1633320129",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
